---
title: "NeurIPS 2024 Competition Track: Auto-Bidding in Large-Scale Auctions: Learning Decision-Making in Uncertain and Competitive Games"
excerpt: "This competition was hosted by the PKU-Alimama Artificial Intelligence Innovation Joint Lab, a collaboration between Alibaba Group 
and Peking University. As a member of the lab, I was actively involved in organizing the competition. I worked closely with Shuai Dou and Yeshu Li to develop the agents 
for the competition system, focusing on creating robust models for decision-making in large-scale, competitive auctions."
collection: portfolio
---

<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research and Development in Reinforcement Learning at Alibaba Group</title>
</head>
<body>
    <h2>Background</h2>
    <p>
        This project was conducted during my internship at Alibaba Group, a globally renowned top 100 company. This internship offered 
        me invaluable exposure to real-world machine learning applications. My initial foray into learning auto-bidding algorithms 
        began at the Center on Frontiers of Computing Studies (CFCS) at Peking University. Through my internship at Alibaba, I was able 
        to apply the algorithms I studied in the lab to practical.
    </p>
    
    <img src="/images/project1.png" alt="Project Overview" style="display: block; margin: 15px auto; max-width: 50%; height: auto;"
    
    <h2>Budget Constrained Bidding Model</h2>
    <p>We consider a bidder participant in a repeated auction for <b>\( T \)</b> rounds.
    We denote by <b>\( B \)</b> and <b>\( R \)</b> the total budget and ROI (return on investment) of the bidder, respectively.</p>
    
    <p>Bidder values the \( t \)-th round auction at valuation \( v_t \), and bid \( b_t \).
    Let \( d_t \) be the winning price for the bidder to win the auction.
    As a result, we can compute the allocation result as \( x_t = 1\{b_t \ge d_t\} \) and the payment as \( p_t = x_t d_t \).</p>
    
    <h3>Optimization Objective</h3>
    
    <p>Our objective is to find the optimal bidding strategy \( \pi \) that bids at round \( t \in [T] \)
    according to the available history \( \mathcal{H}_t = ((v_\tau, b_\tau, p_\tau)_{\tau=1}^{t-1}) \),
    valuation \( v_t \) and budget \( B \). 
    We have \( b_t = \pi(\mathcal{H}_t, v_t, B, R) \), \( x_t = 1\{b_t \ge d_t\} \), \( p_t = x_t d_t \).</p>
    
    <p>The optimization problem is defined as:</p>
    <div class="formula">
        \[
        \begin{aligned}
            \max_{\pi}\quad &\mathbb{E}_{v,\pi}\left[\sum_{t=1}^T x_t v_t\right] \\
            \text{s.t.}\quad &\sum_{t=1}^T x_t p_t \le B,\quad \text{Budget Constraint}
        \end{aligned}
        \]
    </div>
    
    <h3>Constrained MDP Formalization</h3>
    
    <p>We can formalize the above problem as a constrained Markov Decision Process (MDP):</p>
    
    <ul>
        <li><b>State:</b> \( s_t = (\mathcal{H}_t, v_t, B, \{\kappa_j\}_{j\in[K]}) \)</li>
        <li><b>Action:</b> \( b_t \in \mathbb{R}_{\ge 0} \)</li>
        <li><b>Policy:</b> \( b_t = \pi(s_t) \)</li>
        <li><b>Reward:</b> \( R_t = x_t v_t \)</li>
        </li>
    </ul>


    <h2>Project Overview</h2>
    <p>
        My internship was divided into two major components: 
    </p>
    <ol>
        <li>
            <strong>Developing a new Reinforcement Learning Algorithm:</strong>
            Creating a novel reinforcement learning algorithm.
        </li>
        <li>
            <strong>Organizing the NIPS Competition:</strong>
            Co-organizing the “Auto-Bidding in Large-Scale Auctions” competition at NIPS, including developing Official agents for competitive auction environment(<a href="https://h5case6.xiaoxxx.cn/202406/NeurlIPS/dist/index.html#/?lang=en_us" target="_blank" rel="noopener noreferrer">official website</a>).
        </li>
    </ol>

    <h2>Methodology</h2>
    <h3>1. Reinforcement Learning</h3>
    <p>
        I will disclose the details of my research after the double-blind review.
    </p>

    <h3>2. Organizing the NIPS Competition</h3>
    <p>
        As part of the organizing team for the NIPS competition, I developed reinforcement learning agents capable of participating in large-scale, competitive, real-time auction environments. The system required designing agents with robust auto-bidding capabilities under high uncertainty. My contributions included:
    </p>
    <ul>
        <li>
            Training agents using advanced offline reinforcement learning techniques, including BCQ, CQL, and IQL.
        </li>
        <li>
            Incorporating Google DeepMind's Q-value optimization tricks for industrial applications to improve agent stability and performance.
        </li>
        <li>
            My BCQ agent achieving third place among all baseline agents in the testing phase.
        </li>
    </ul>

    <h2>Key Achievements</h2>
    <ol>
        <li>
            <strong>Practical Impact:</strong>
            Contributed to the NIPS competition as a key organizer and developer, creating robust and competitive baseline agents for participants to challenge.
        </li>
    </ol>

    <h2>Challenges and Reflections</h2>
    <p>
        This internship presented numerous challenges that significantly advanced my skills and understanding of reinforcement learning:
    </p>
    <ol>
        <li>
            <strong>Algorithm Design Complexity:</strong>
            Developing a completely new reinforcement learning algorithm required extensive experimentation and iteration. Over a half-year period, I dedicated myself to continuous experimentation, even setting up a tent near my workstation to ensure uninterrupted model training.
        </li>
        <li>
            <strong>Competency in Safe Reinforcement Learning:</strong>
            Designing agents for a competitive and highly uncertain environment required learning and applying safety constraints in reinforcement learning. These efforts ensured agent stability and effectiveness during competition.
        </li>
    </ol>

</body>
</html>
